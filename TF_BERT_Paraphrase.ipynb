{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-BERT-Paraphrase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neKlYk1fcX7K",
        "outputId": "e9ce94e7-af8f-4814-825b-befda1917976"
      },
      "source": [
        "pip install -q tf-models-official==2.4.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 68.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.1 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 211 kB 63.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 12.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 52.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.6 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cs0kQhfcmvt"
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from ast import literal_eval\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "from os import listdir\n",
        "\n",
        "#required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjI3PhQCczdF",
        "outputId": "e7cc21cd-3920-4d8d-b4fb-8dfbd4ad3d54"
      },
      "source": [
        "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12\"\n",
        "tf.io.gfile.listdir(gs_folder_bert)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bert_config.json',\n",
              " 'bert_model.ckpt.data-00000-of-00001',\n",
              " 'bert_model.ckpt.index',\n",
              " 'vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBHX3lVjdBJa",
        "outputId": "383059cf-7bd1-43cc-b57c-baa1e0f86ab4"
      },
      "source": [
        "#Microsoft Research Paraphrase Corpus\n",
        "glue, info = tfds.load('glue/mrpc', with_info=True, batch_size=-1)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiRqUeNidEnc",
        "outputId": "3582e4ce-f707-403a-dcc1-6cbb81d04240"
      },
      "source": [
        "list(glue.keys())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['test', 'train', 'validation']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8zqXR-PdKQr",
        "outputId": "3ea51089-ef33-4fe7-e1d3-43e90b92f879"
      },
      "source": [
        "info.features"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeaturesDict({\n",
              "    'idx': tf.int32,\n",
              "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "    'sentence1': Text(shape=(), dtype=tf.string),\n",
              "    'sentence2': Text(shape=(), dtype=tf.string),\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-KOz5_-dMmP",
        "outputId": "1c0cc682-5b67-4daa-9d35-a28aeda46acd"
      },
      "source": [
        "info.features['label'].names"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['not_equivalent', 'equivalent']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL7rpsNgdPIf",
        "outputId": "ea71793b-1ca6-472c-81a0-212e91709a11"
      },
      "source": [
        "glue_train = glue['train']\n",
        "\n",
        "for key, value in glue_train.items():\n",
        "  print(f\"{key:9s}: {value[0].numpy()}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idx      : 1680\n",
            "label    : 0\n",
            "sentence1: b'The identical rovers will act as robotic geologists , searching for evidence of past water .'\n",
            "sentence2: b'The rovers act as robotic geologists , moving on six wheels .'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2ewfdiZdS7F",
        "outputId": "bc59b98e-4f37-4cb4-d686-41be9ca70dae"
      },
      "source": [
        "#rebuild vocab used by base bert\n",
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"), do_lower_case=True)\n",
        "\n",
        "print(\"Vocab size:\", len(tokenizer.vocab))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 30522\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqMhxPNedhPG",
        "outputId": "278cf14e-846f-45ff-9bba-7b417e705849"
      },
      "source": [
        "#the inputs will be concatenated tg with CLS and SEP tokens starting and seperating\n",
        "tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2eKb-cDd96W"
      },
      "source": [
        "def encode_sentence(s, tokenizer):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def bert_encode(glue_dict, tokenizer):\n",
        "  num_examples = len(glue_dict[\"sentence1\"])\n",
        "\n",
        "  sentence1 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence1\"])])\n",
        "  \n",
        "  sentence2 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence2\"])])\n",
        "\n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
        "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s1 = tf.zeros_like(sentence1)\n",
        "  type_s2 = tf.ones_like(sentence2)\n",
        "  input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'input_type_ids': input_type_ids}\n",
        "\n",
        "  return inputs\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbt5Vb9peCd_"
      },
      "source": [
        "glue_train, glue_train_labels = bert_encode(glue['train'], tokenizer), glue['train']['label']\n",
        "\n",
        "glue_validation, glue_validation_labels = bert_encode(glue['validation'], tokenizer), glue['validation']['label']\n",
        "\n",
        "glue_test, glue_test_labels = bert_encode(glue['test'], tokenizer), glue['test']['label']\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUC6Sf_LeH5A",
        "outputId": "79fa51aa-6d03-475f-8749-b02cd4b47e6f"
      },
      "source": [
        "for key, value in glue_train.items():\n",
        "  print(f'{key:15s} shape: {value.shape}')\n",
        "\n",
        "print(f'glue_train_labels shape: {glue_train_labels.shape}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_word_ids  shape: (3668, 103)\n",
            "input_mask      shape: (3668, 103)\n",
            "input_type_ids  shape: (3668, 103)\n",
            "glue_train_labels shape: (3668,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGDObQGYeIny",
        "outputId": "73ed0072-25d1-4f48-c7a2-fa1e26b41e0c"
      },
      "source": [
        "import json\n",
        "\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"bert_config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
        "\n",
        "config_dict"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.1,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.1,\n",
              " 'hidden_size': 768,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'max_position_embeddings': 512,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_hidden_layers': 12,\n",
              " 'type_vocab_size': 2,\n",
              " 'vocab_size': 30522}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qbUMof7eIqq"
      },
      "source": [
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(bert_config, num_labels=2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRY9y1_2eZV1",
        "outputId": "0a333c04-8145-457b-81bf-40a76fbc54dd"
      },
      "source": [
        "#restore weights from checkpoint\n",
        "checkpoint = tf.train.Checkpoint(encoder=bert_encoder)\n",
        "checkpoint.read(os.path.join(gs_folder_bert, 'bert_model.ckpt')).assert_consumed()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f03b5b41fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwuaJTomeeGy"
      },
      "source": [
        "# Set up epochs and steps\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "train_data_size = len(glue_train_labels)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5, \n",
        "    num_train_steps = steps_per_epoch * epochs, \n",
        "    num_warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "                                              )\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPacF2wOeiOa",
        "outputId": "332f7e36-5163-42bc-d1ae-6d95a057eb47"
      },
      "source": [
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "bert_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "bert_classifier.fit(\n",
        "      glue_train, glue_train_labels,\n",
        "      validation_data=(glue_validation, glue_validation_labels),\n",
        "      batch_size=32,\n",
        "      epochs=epochs)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "115/115 [==============================] - 90s 673ms/step - loss: 0.6355 - accuracy: 0.6104 - val_loss: 0.5090 - val_accuracy: 0.7672\n",
            "Epoch 2/3\n",
            "115/115 [==============================] - 75s 656ms/step - loss: 0.4556 - accuracy: 0.7887 - val_loss: 0.4429 - val_accuracy: 0.8260\n",
            "Epoch 3/3\n",
            "115/115 [==============================] - 75s 656ms/step - loss: 0.3439 - accuracy: 0.8509 - val_loss: 0.4366 - val_accuracy: 0.8358\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f03b5b20390>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}